{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Intuition:\n",
    "\n",
    "We've already discussed how a basic perceptron model works. In order to review it, in a basic perceptron model we have a __Neuron__ which has some __Inputs__ and __Weights__ associated with it. The __Inputs__ get multiplied with the __Weights__ and there is some biasing factor added in the product. Then we apply some sort of __activation function__ to get the final output of the __Neuron__.\n",
    "\n",
    "Now, there's is still some theory components which we need to cover in order to understand __CNN__. So let's go ahead and cover some of these components.\n",
    "\n",
    "## Initialization of Weights:\n",
    "* Zeros: Choosing __Zeros__ for weights values, is not a great choice, cuz there's no randomness there\n",
    "* Random distribution near Zero: Not optimal. When passed in to __Activation function__ sometimes get distorted to much larger values.\n",
    "\n",
    "* Xavier(Glorot) Initialization: Comes in both __Uniform__ and __Normal__ distributions. In case of __Xavier Initialization__, the parameters are initialized as __random draws__ from a __normal distribution__ with a __mean zero__ and __variance = 2 / (no. of inputs + no. of outputs)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate: \n",
    "Defines the step size during __Gradient Descent__. Choosing small learning rate, leads the __optimizer__ to descent at very very slow pace, and may take forever to actually train your model. Or if we choose too large learning rate, we may __overshoot__ the convergence point. So, we need to choose an intermediate value of for learning rate, which is also an important component.\n",
    "\n",
    "## Batch Size:\n",
    "Allows us to use __Stochastic Gradient Descent__. \n",
    "* Smaller -> Less representative of data\n",
    "* Larger -> Longer training time / Computationally expensive\n",
    "\n",
    "## Second Order Behaviour of Descent:\n",
    "Allows us to adjust our learning rate based off the rate of descent.\n",
    "* AdaGrad\n",
    "* RMSProp\n",
    "* Adam\n",
    "* Annealing\n",
    "These all gradient methods allows us to descent at faster rate, and as soon as we get closer to __Global Minimum__ it automatically truncates the descent rate, in order to not __overshoot__ the convergence point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstable / Vanishing Gradient:\n",
    "\n",
    "As we increase the number of layers in our __Neural Network__ the derivate of __error__ with respect to the __weight__ parameters becomes lower and lower. This leads to the very small amount of change in descent while propagating backwards, and the __input layers__ may not learn at a specific point. This issue is called __Gradient Vanishing__.\n",
    "\n",
    "So, __Initialization__ and __Normalization__ will help us mitigate these issues. We'll discuss these issue in more detail in __RNNs__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting Problem:\n",
    "Many a times it happens that, our training model learns very well on training dataset, but when tested on test dataset, it comes up with large amount of errors. This is due to overfittig, in which model learns very well bisecting almost every data point, but fails on test dataset. This issue mostly come where, we have large number of features, like __image classification__ or __CNNs__. To avoid this issue, we have following methods:\n",
    "\n",
    "## Regularization:\n",
    "In Regularization technique, we tend to add some __panelty__ for the __weights__ or shrink down some of the __weights__ to drop some of the features (mostly those which are less important) to reduce the problem of overfitting.\n",
    "\n",
    "## Dropout:\n",
    "This technique basically removes the neurons during training randomly (mostly those which are less important) so that network doesn't over rely on any particular neuron. And that can also help mitigating the __overfitting__ issue.\n",
    "\n",
    "## Expanding Data:\n",
    "In this technique we __Artificially expand data__ by adding noise, tilting images and adding low white noise to sound data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset:\n",
    "Tensorflow has built-in dataset called __MNIST__ which contains the hand written images or numbers from 0-9. We can use __Softmax Regression__ or __CNN__ to predict the image using training dataset. We'll do this, first by using __Softmax Regression__ and then __CNN__.\n",
    "\n",
    "What __Softmax Function__ is, it simply returns back the probabilities which if summed gives result equals 1.\n",
    "\n",
    "Let's go ahead and implement it, using __Python__ and __Tensorflow__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
